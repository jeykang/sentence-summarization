Sentence compression is a standard NLP task where the goal is to generate a shorter paraphrase of a sentence.
Dozens of systems have been introduced in the past two decades and most of them are deletion-based: generated compressions are token subsequences of the input sentences

Automatic sentence compression can be broadly described as the task of creating a grammatical summary of a single sentence with minimal information loss.
It has recently attracted much attention, in part because of its relevance to applications. Examples include the generation of subtitles from spoken transcripts

Summarization in general, and sentence compression in particular, are popular topics.
Knight and Marcu (henceforth K&M) introduce the task of statistical sentence compression in Statistics-Based Summarization - Step One: Sentence Compression.
The appeal of this problem is that it produces summarizations on a small scale. It simplifies general compression problems, such as text-to-abstract conversion, by eliminating the need for coherency between sentences.
The model is further simplified by being constrained to word deletion: no rearranging of words takes place.
Others have performed the sentence compression task using syntactic approaches to this problem, but we focus exclusively on the K&M formulation.
Though the problem is simpler, it is still pertinent to current needs; generation of captions for television and audio scanning services for the blind, as well as compressing chosen sentences for
headline generation are examples of uses for sentence compression.
In addition to simplifying the task, K&M¡¯s noisy-channel formulation is also appealing.

The ability to compress sentences grammatically with minimal information loss is an important problem in text summarization.
Most summarization systems are evaluated on the amount of relevant information retained as well as their compression rate.
Thus, returning highly compressed, yet informative, sentences allows summarization systems to return larger sets of sentences and increase the overall amount of information extracted

Most of the research in automatic summarization has focused on extraction, i.e., on identifying the most important clauses/sentences/paragraphs in texts.
However, determining the most important textual segments is only half of what a summarization system needs to do because, in most cases, the simple catenation of textual segments does not yield coherent outputs.
Recently, a number of researchers have started to address the problem of generating coherent summaries: McKeown et al. (1999), Barzilay et al. (1999), and Jing and McKeown (1999) in the context of multidocument summarization; Mani et al. (1999) in the context of revising single document extracts; and Witbrock and Mittal (1999) in the context of headline generation